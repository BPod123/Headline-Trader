{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import tqdm\n",
    "from BOW_data_format import prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "prepared = prepare_data()\n",
    "title_data = prepared['title_data']\n",
    "train_data = prepared['train']\n",
    "validation_data = prepared['validation']\n",
    "test_data = prepared['test']\n",
    "vocab = prepared['vocab']\n",
    "PAD_ID = prepared['PAD_ID']\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "titles = nn.utils.rnn.pad_sequence([torch.LongTensor(title_data[i]) if i in title_data else torch.LongTensor([PAD_ID]) for i in range(max(title_data) + 1)], batch_first=True).to(device).contiguous()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Data Loaders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def bag_of_words_collate(batch):\n",
    "    batch_arr = np.array(batch).T\n",
    "    data = titles[batch_arr[0]]\n",
    "    labels = torch.FloatTensor(batch_arr[1]).to(device)\n",
    "    return data, labels.squeeze()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, sampler=RandomSampler(train_data), collate_fn=bag_of_words_collate)\n",
    "validation_iter = DataLoader(validation_data, sampler=RandomSampler(validation_data), batch_size=1, collate_fn=bag_of_words_collate)\n",
    "test_iter = DataLoader(test_data, sampler=RandomSampler(test_data), batch_size=1, collate_fn=bag_of_words_collate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make the Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "class BagOfWords(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, dropout=0.1):\n",
    "        super(BagOfWords, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(embedding_dim, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        out = self.dropout(self.embedding(x))\n",
    "        out = out.mean(1)\n",
    "        out = self.activation(self.linear(out)).squeeze()\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def eval_model(model, data_iter):\n",
    "    model.eval()\n",
    "    predictions, all_labels= [], []\n",
    "    for (data, labels) in data_iter:\n",
    "        out = model(data)\n",
    "        predictions.append(out.unsqueeze(0))\n",
    "        all_labels.append(labels.unsqueeze(0))\n",
    "    pred = torch.cat(predictions, axis=-1).squeeze().round().to('cpu')\n",
    "    true = torch.cat(all_labels, axis=-1).squeeze().to('cpu')\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    f1_scores = []\n",
    "    for selected_class in range(2):\n",
    "        tp = ((pred == selected_class) & (true == selected_class)).sum()\n",
    "        fp = ((pred == selected_class) & (true != selected_class)).sum()\n",
    "        fn = ((pred != selected_class) & (true == selected_class)).sum()\n",
    "        recall = tp / (tp + fn) if tp + fn != 0 else 0\n",
    "        precision = tp / (tp + fp) if tp + fp != 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if precision + recall != 0 else 0\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        f1_scores.append(f1)\n",
    "    return pred, true, recalls, precisions, f1_scores\n",
    "\n",
    "def eval_on_test_set(model):\n",
    "    _, _, recalls, precisions, f1_scores = eval_model(model, test_iter)\n",
    "    print(f\"\"\"Run on Test Data:\n",
    "    Down: Recall: {recalls[0]}\\tPrecision: {precisions[0]}\\tF1: {f1_scores[0]}\n",
    "    Up: Recall: {recalls[1]}\\tPrecision: {precisions[1]}\\tF1: {f1_scores[1]}\n",
    "    AVERAGE: Recall: {sum(recalls) / 2}\\tPrecision: {sum(precisions) / 2}\\tF1: {sum(f1_scores) / 2}\"\"\")\n",
    "\n",
    "def eval_summary(epoch):\n",
    "        _, _, recalls, precisions, f1_scores = eval_model(model, validation_iter)\n",
    "        print(f\"\"\"Epoch {epoch} Validation:\n",
    "Down: Recall: {recalls[0]}\\tPrecision: {precisions[0]}\\tF1: {f1_scores[0]}\n",
    "Up: Recall: {recalls[1]}\\tPrecision: {precisions[1]}\\tF1: {f1_scores[1]}\n",
    "AVERAGE: Recall: {sum(recalls) / 2}\\tPrecision: {sum(precisions) / 2}\\tF1: {sum(f1_scores) / 2}\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def train_model(model, data_iter, epochs, optimizer, scheduler, loss_func):\n",
    "    epoch_average_losses = []\n",
    "    with tqdm.notebook.trange(epochs, desc='training', unit='epoch') as epoch_iter:\n",
    "        for epoch in epoch_iter:\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            epoch_samples = 0\n",
    "            with tqdm.notebook.tqdm(data_iter, desc=f\"epoch {epoch + 1}\", unit='batch', total=len(data_iter)) as batch_iter:\n",
    "                for i, (data, labels) in enumerate(batch_iter, start=1):\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(data)\n",
    "                    loss = loss_func(output, labels.squeeze())\n",
    "                    epoch_loss += loss.item()\n",
    "                    epoch_samples += len(labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    batch_iter.set_postfix(mean_epoch_loss=epoch_loss / i)\n",
    "                if epoch % 10 == 0:\n",
    "                    eval_summary(epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            avg_epoch_loss = epoch_loss / epoch_samples\n",
    "            scheduler.step(avg_epoch_loss)\n",
    "            epoch_average_losses.append(avg_epoch_loss)\n",
    "\n",
    "    return epoch_average_losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "EPOCHS = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre Training Stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on Test Data:\n",
      "    Down: Recall: 1.0\tPrecision: 0.5098039507865906\tF1: 0.6753246784210205\n",
      "    Up: Recall: 0.0\tPrecision: 0\tF1: 0\n",
      "    AVERAGE: Recall: 0.5\tPrecision: 0.2549019753932953\tF1: 0.33766233921051025\n",
      "\n",
      "\n",
      "Epoch 0 Validation:\n",
      "Down: Recall: 1.0\tPrecision: 0.71875\tF1: 0.8363636136054993\n",
      "Up: Recall: 0.0\tPrecision: 0\tF1: 0\n",
      "AVERAGE: Recall: 0.5\tPrecision: 0.359375\tF1: 0.41818180680274963\n"
     ]
    }
   ],
   "source": [
    "model = BagOfWords(len(vocab), 1000).to(device)\n",
    "eval_on_test_set(model)\n",
    "print('\\n')\n",
    "eval_summary(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "training:   0%|          | 0/10 [00:00<?, ?epoch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26551a6c4a1945df9bf2884131c3b137"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "epoch 1:   0%|          | 0/99 [00:00<?, ?batch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b7d1fb6bcc3486680df5df3d4418b99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation:\n",
      "Down: Recall: 0.0\tPrecision: 0\tF1: 0\n",
      "Up: Recall: 1.0\tPrecision: 0.28125\tF1: 0.4390243887901306\n",
      "AVERAGE: Recall: 0.5\tPrecision: 0.140625\tF1: 0.2195121943950653\n"
     ]
    },
    {
     "data": {
      "text/plain": "epoch 2:   0%|          | 0/99 [00:00<?, ?batch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e516819c71fa422897fe3d09295d76a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "epoch 3:   0%|          | 0/99 [00:00<?, ?batch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1cd5a024dd28476aaeb248512283eaba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "epoch 4:   0%|          | 0/99 [00:00<?, ?batch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c6cbc5a9d064141bda5eb8594e54e6b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "epoch 5:   0%|          | 0/99 [00:00<?, ?batch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22d9d0989acb4dc49cbfe68aa60f16b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "epoch 6:   0%|          | 0/99 [00:00<?, ?batch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ef748369a294bee8aeebdf567c8d57f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "epoch 7:   0%|          | 0/99 [00:00<?, ?batch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "df7c34c87c87458b82b6e3b2d124c2ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "epoch 8:   0%|          | 0/99 [00:00<?, ?batch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c2b38e390c24fe09db6f8009b14f62c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "epoch 9:   0%|          | 0/99 [00:00<?, ?batch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80c965654ecb482da663b99078ed0724"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "epoch 10:   0%|          | 0/99 [00:00<?, ?batch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd275b5375b54297981e487564480b0c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_func = nn.BCELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=3, threshold=0.01)\n",
    "history = train_model(model, train_iter, EPOCHS, optimizer, scheduler, loss_func)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isdir('../Saved_Models'):\n",
    "    os.mkdir('../Saved_Models')\n",
    "torch.save(model, '../Saved_Models/bag_of_words.pt')\n",
    "bag = torch.load('../Saved_Models/bag_of_words.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Post Training Stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Validation:\n",
      "Down: Recall: 0.0\tPrecision: 0\tF1: 0\n",
      "Up: Recall: 1.0\tPrecision: 0.28125\tF1: 0.4390243887901306\n",
      "AVERAGE: Recall: 0.5\tPrecision: 0.140625\tF1: 0.2195121943950653\n",
      "\n",
      "\n",
      "Run on Test Data:\n",
      "    Down: Recall: 0.0\tPrecision: 0\tF1: 0\n",
      "    Up: Recall: 1.0\tPrecision: 0.4901960790157318\tF1: 0.6578947305679321\n",
      "    AVERAGE: Recall: 0.5\tPrecision: 0.2450980395078659\tF1: 0.32894736528396606\n"
     ]
    }
   ],
   "source": [
    "eval_summary(EPOCHS)\n",
    "print('\\n')\n",
    "eval_on_test_set(bag)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Write results to CSV"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def run_on_data(model, data_iter):\n",
    "    model.eval()\n",
    "    predictions, all_labels= [], []\n",
    "    for (data, labels) in data_iter:\n",
    "        out = model.predict_proba(data)\n",
    "        predictions.append(out.unsqueeze(0))\n",
    "        all_labels.append(labels.unsqueeze(0))\n",
    "    pred = torch.cat(predictions, axis=-1).squeeze()\n",
    "    true = torch.cat(all_labels, axis=-1).squeeze()\n",
    "    return pred, true\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "train_pred, train_labels = run_on_data(model, train_iter)\n",
    "valid_pred, valid_labels = run_on_data(model, validation_iter)\n",
    "test_pred, test_labels = run_on_data(model, test_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_results = pd.DataFrame({\"Labels\": train_labels.to('cpu'), \"Predictions\": train_pred.to('cpu').detach().numpy()})\n",
    "#, \"Date\": #train_data['Date']})\n",
    "valid_results = pd.DataFrame({\"Labels\": valid_labels.to('cpu'), \"Predictions\": valid_pred.to('cpu').detach().numpy()})\n",
    "#, \"Date\": validation_data['Date']})\n",
    "test_results = pd.DataFrame({\"Labels\": test_labels.to('cpu'), \"Predictions\": test_pred.to('cpu').detach().numpy()})\n",
    "#, \"Date\": test_data['Date']})\n",
    "\n",
    "train_results.to_csv('../Results/train/BOW.csv', index=False)\n",
    "valid_results.to_csv('../Results/validation/BOW.csv', index=False)\n",
    "test_results.to_csv('../Results/test/BOW.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}